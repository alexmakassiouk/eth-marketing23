{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from mpmath import nsum, exp, inf\n",
    "from main import get_all_retention_rates_for_cohort, get_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.9915769295055\n"
     ]
    }
   ],
   "source": [
    "data = get_data()\n",
    "\n",
    "retention_rate = 0.56530017\n",
    "\n",
    "#E[lifetime] = 1*retention_rate + 2*retention_rate^2 +..n*retention_rate^3\n",
    "\n",
    "print(nsum(lambda n: n*retention_rate**n, [1, inf]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">>Should develop strategies to keep the customers in the second month"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The expected lifetime of above 2 is heavily influenced by the loyal customers, that have been staying for many months and therefore driving the expected value up. On the other side we have seen big drops of customers after only one or two months, and we would therefore want to get more users past that critical point of two months. This could be done by more effectively showing the customer relevant content. It is however difficult to personalize content for a user that they have little data on, and one strategy could therefore be to gather data about the particular user from outside of Netflix itself to propose relevant, personalized content that may make the user stay past the critical point"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.48308715 0.05753459 0.01415235 0.00602463 0.0026858  0.00161148\n",
      " 0.00053716 0.00053716 0.00053716 0.00053716]\n",
      "0.7060732139754536\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/lib/python3.10/site-packages/numpy/lib/shape_base.py:379: RuntimeWarning: Mean of empty slice\n",
      "  res = asanyarray(func1d(inarr_view[ind0], *args, **kwargs))\n",
      "/opt/homebrew/lib/python3.10/site-packages/numpy/lib/shape_base.py:402: RuntimeWarning: Mean of empty slice\n",
      "  buff[ind] = asanyarray(func1d(inarr_view[ind], *args, **kwargs))\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def get_all_retention_rates_time_normalized(data):\n",
    "    retention_rates_all_cohorts = []\n",
    "    for i in range(12):\n",
    "        all_retention_rates_for_cohort = (get_all_retention_rates_for_cohort(data,i))\n",
    "        retention_rates_all_cohorts.append(all_retention_rates_for_cohort)\n",
    "    for j in range(1,len(retention_rates_all_cohorts)):\n",
    "        for i in range(j,len(retention_rates_all_cohorts[j])):\n",
    "            retention_rates_all_cohorts[j][i-j] = retention_rates_all_cohorts[j][i]\n",
    "            retention_rates_all_cohorts[j][i] = np.NaN\n",
    "    return retention_rates_all_cohorts\n",
    "\n",
    "get_all_retention_rates_months_after_acquisition = get_all_retention_rates_time_normalized(data)\n",
    "\n",
    "avg_monthly_retention_rate_across_cohorts = np.apply_along_axis(np.nanmean,axis = 0, arr = get_all_retention_rates_months_after_acquisition)\n",
    "\n",
    "\n",
    "retention_probabilities = np.cumprod(avg_monthly_retention_rate_across_cohorts[1:-1])\n",
    "print(retention_probabilities)\n",
    "\n",
    "expected_value = 0\n",
    "i=1\n",
    "for prob in list(retention_probabilities):\n",
    "    expected_value += prob*i\n",
    "    i+=1\n",
    "print(expected_value)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7060732139754536"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "expectation_value = np.arange(1,len(retention_probabilities)+1,1)@retention_probabilities\n",
    "expectation_value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The values for the different methods of calculating are radically different. Similarly as before we see here a sort of \"abuse\" of doing the average of averages in eliminating the relevance of the sample size. In this case in 4a we are using the average retention rate of the average retention rates of each month after customer acquisition.\n",
    "To get the relevance of all customers, also the ones that churned early, the last retention rate in 4c should be used as it is more realistic.\n",
    "Using this, we realize that many churn after only one month, and the active probabilities fall quickly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "#4d\n",
    "#Strategies for keeping customers during first month vs second month"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
