{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from mpmath import nsum, exp, inf\n",
    "from main import get_all_retention_rates_for_cohort, get_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- $p_{c_k}$ : Active probability from constant retention rate. $p_{c_1} = 1$\n",
    "- $r_c$ : Constant Retention rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\mathbb{E}_c [user \\space months] = \\sum_{k=1}^{\\inf}k \\cdot p_{c_{k}} = \\sum_{k=1}^{\\inf}k \\cdot r_c^{k-1}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.29201491219346\n"
     ]
    }
   ],
   "source": [
    "data = get_data()\n",
    "\n",
    "constant_retention_rate = 0.56530017\n",
    "\n",
    "active_probs_constant_retention = [constant_retention_rate**i for i in range(1000)]\n",
    "#E[lifetime] = 1*retention_rate + 2*retention_rate^2 +..n*retention_rate^3\n",
    "# print(active_probs_constant_retention)\n",
    "# print(nsum(lambda n: n*active_probs_constant_retention[n-1], [1, 100]))\n",
    "expected_value_constant_retention = sum([i*active_probs_constant_retention[i-1] for i in range(1,len(active_probs_constant_retention))])\n",
    "print(expected_value_constant_retention)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**OLD:**\n",
    "\n",
    "\n",
    "*Should develop strategies to keep the customers in the second month*\n",
    "\n",
    "\n",
    "*The expected lifetime of above 2 is heavily influenced by the loyal customers, that have been staying for many months and therefore driving the expected value up. On the other side we have seen big drops of customers after only one or two months, and we would therefore want to get more users past that critical point of two months. This could be done by more effectively showing the customer relevant content. It is however difficult to personalize content for a user that they have little data on, and one strategy could therefore be to gather data about the particular user from outside of Netflix itself to propose relevant, personalized content that may make the user stay past the critical point*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$5.29$ is a rather high expected lifetime for customers. This is using the constant retention rate and assuming everyone finishes at least one month. The math looks something like $1 + 2\\cdot 0.57^1 + 3 \\cdot 0.57^2 + 4 \\cdot 0.57^3  \\cdots = 1 + 1.14 + 0.97 + 0.74 \\cdots $. We have already discussed that the average retention rate is artificially high, and so this expected value seems high as well as a consequence. It makes however sence if we consider the long-serving customers and not only the ones acquired in our time window of data. And so this expected value rather represents one type of the general customer base which are the long-serving ones"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[       nan 0.48308715 0.11909775 0.24597981 0.42569815 0.44580342\n",
      " 0.6        0.33333333 1.         1.         1.                nan]\n",
      "[1.00000000e+00 4.83087149e-01 5.75345930e-02 1.41523483e-02\n",
      " 6.02462849e-03 2.68580000e-03 1.61148000e-03 5.37160000e-04\n",
      " 5.37160000e-04 5.37160000e-04 5.37160000e-04]\n",
      "2.273317852896713\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def get_all_retention_rates_time_normalized(data):\n",
    "    retention_rates_all_cohorts = []\n",
    "    for i in range(12):\n",
    "        all_retention_rates_for_cohort = (get_all_retention_rates_for_cohort(data,i))\n",
    "        retention_rates_all_cohorts.append(all_retention_rates_for_cohort)\n",
    "    for j in range(1,len(retention_rates_all_cohorts)):\n",
    "        for i in range(j,len(retention_rates_all_cohorts[j])):\n",
    "            retention_rates_all_cohorts[j][i-j] = retention_rates_all_cohorts[j][i]\n",
    "            retention_rates_all_cohorts[j][i] = np.NaN\n",
    "    return retention_rates_all_cohorts\n",
    "\n",
    "get_all_retention_rates_months_after_acquisition = get_all_retention_rates_time_normalized(data)\n",
    "\n",
    "avg_monthly_retention_rate_across_cohorts = np.apply_along_axis(np.nanmean,axis = 0, arr = get_all_retention_rates_months_after_acquisition)\n",
    "print(avg_monthly_retention_rate_across_cohorts)\n",
    "\n",
    "active_probs_changing_retention = avg_monthly_retention_rate_across_cohorts[1:-1]\n",
    "active_probs_changing_retention = np.insert(active_probs_changing_retention,0,1)\n",
    "active_probs_changing_retention = np.cumprod(active_probs_changing_retention)\n",
    "# active_probs_changing_retention = np.append(active_probs_changing_retention,np.NaN)\n",
    "print(active_probs_changing_retention)\n",
    "\n",
    "expected_value = 0\n",
    "i=1\n",
    "for prob in list(active_probs_changing_retention):\n",
    "    expected_value += prob*i\n",
    "    i+=1\n",
    "\n",
    "print(expected_value)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.273317852896713"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "expectation_value = np.arange(1,len(active_probs_changing_retention)+1,1)@active_probs_changing_retention\n",
    "expectation_value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The values for the different methods of calculating are radically different. Similarly as before we see here a sort of \"abuse\" of doing the average of averages in eliminating the relevance of the sample size. In this case in 4a we are using the average retention rate of the average retention rates of each month after customer acquisition.\n",
    "To get the relevance of the newly acquired customers, also the ones that churned early, the last retention rate in 4c should be used as it is more realistic.\n",
    "Using this, we realize that many churn after only one month, and the active probabilities fall quicker then in the first case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "#4d\n",
    "#Strategies for keeping customers during first month vs second month"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
